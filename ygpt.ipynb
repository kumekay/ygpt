{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3702a9ba-57a5-4b5c-be2f-413390653298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "![[ ! -f './input.txt' ]] && wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e286d3b9-0906-482e-8bca-b166a0a8cd8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ee4f208-105e-47ef-827d-4c54af1bf42f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_path = Path(\"./input.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f633e524-e338-4df8-92d3-a21e756ee690",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = text_path.read_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfcb832c-8c3b-4444-849d-ed09e0534df1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0ed6e5c-3266-4768-bcd2-66ad5ec15757",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(text_path.absolute().parent.parent / \"project_b\").exists()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e546f537-4249-4661-825d-38cdd2e7d757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "voc = sorted(set(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e7ad1b6-7735-479e-b8af-bab59b1e1764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "voc_len = len(voc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ffad911-e730-4e03-89f8-72ffa9378896",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2649b7a-58c5-40f7-b6ab-dad2013542c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41, 53, 41, 53, 1, 48, 59, 51, 40, 53] coco jumbo\n"
     ]
    }
   ],
   "source": [
    "i2s = dict(enumerate(voc))\n",
    "s2i = {v: k for (k, v) in i2s.items()}\n",
    "\n",
    "\n",
    "def encode(s: str) -> list[int]:\n",
    "    return [s2i[k] for k in s]\n",
    "\n",
    "\n",
    "def decode(i: list[int]) -> str:\n",
    "    return \"\".join([i2s[k] for k in i])\n",
    "\n",
    "\n",
    "a = encode(\"coco jumbo\")\n",
    "print(a, decode(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "909f6863-b3ce-4fb3-864f-ac0a1d1e9f44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e5e9380-a84d-44a2-aef9-237ec9615b75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "126c31a5-6aad-4c42-97cf-203c587ca811",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f46a287-98af-4250-80c9-754f8a461222",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "090a48df-cec2-4992-833e-84b3710ba1c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "385e77b7-1c8a-4675-a259-7640f4403508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = int(len(data) * 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fdab51b-64a4-421b-af06-7836e6458952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = data[:split]\n",
    "val = data[split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29aeafb6-22f7-4966-b32c-844edfc2a8ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train[: block_size + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "33394ef6-54a2-48df-b032-d713a0cb11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_batch(is_train: bool = True):\n",
    "    data = train if is_train else val\n",
    "    ids = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ids])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ids])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ddb3f8b-e477-4676-bcee-b6039c62c8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0, 32, 46, 53, 59,  1, 42, 43],\n",
       "         [46, 43,  1, 56, 43, 57, 58,  1],\n",
       "         [58, 47, 52, 45,  1, 52, 53,  1],\n",
       "         [57, 12,  1, 21, 57,  1, 58, 46]]),\n",
       " tensor([[32, 46, 53, 59,  1, 42, 43, 57],\n",
       "         [43,  1, 56, 43, 57, 58,  1, 53],\n",
       "         [47, 52, 45,  1, 52, 53,  1, 51],\n",
       "         [12,  1, 21, 57,  1, 58, 46, 47]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e3dcc2a2-437a-4759-87c4-37b0329e0ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65]) tensor(4.6485, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as fn\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # (batch_size, block_size, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.reshape(B * T, C)\n",
    "        targets = targets.reshape(B * T)\n",
    "\n",
    "        loss = fn.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens=100):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self.forward(idx)\n",
    "            logits = logits[:, -1, :] # (batch_size, vocab_size)\n",
    "            probs = fn.softmax(logits, dim=-1) # (batch_size, vocab_size)\n",
    "            idx_next = torch.multinomial(probs, 1) # (batch_size, 1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1) # (batch_size, T + 1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(voc_len)\n",
    "m = m.to(device)\n",
    "\n",
    "logits, loss = m(*get_batch())\n",
    "print(logits.shape, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1a8ee556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nS']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[decode(i) for i in  m.generate(torch.tensor([[0]]), max_new_tokens=1).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "644936ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "51a19d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss 4.656\n",
      "step 1000: loss 3.724\n",
      "step 2000: loss 3.162\n",
      "step 3000: loss 2.912\n",
      "step 4000: loss 2.654\n",
      "step 5000: loss 2.620\n",
      "step 6000: loss 2.489\n",
      "step 7000: loss 2.531\n",
      "step 8000: loss 2.547\n",
      "step 9000: loss 2.543\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for step in range(10000):\n",
    "    xb, yb = get_batch()\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"step {step}: loss {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "af6a10cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Ang\n",
      "Brd tule t\n",
      "Coue\n",
      "\n",
      "e, pakl illferothe, wed\n",
      "Y:\n",
      "Bu mmblors, k\n",
      "n trs brind pis MO:\n",
      "LARA thcareroous\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.tensor([[0]])).tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eb0225d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 2.3936, -5.1045, -5.3803,  ..., -3.5042, -1.7965, -5.8322],\n",
       "         [-4.3490, -3.9531, -5.1122,  ..., -6.4487,  0.6181, -3.1308],\n",
       "         [ 2.6919,  2.2219, -2.9095,  ..., -3.9925, -4.4914, -2.4891],\n",
       "         ...,\n",
       "         [-0.9544, -1.0100, -3.7172,  ..., -2.2793, -1.0173, -2.1112],\n",
       "         [-0.5053,  2.7796, -1.2012,  ..., -3.7570, -4.9620, -4.8453],\n",
       "         [-3.5804, -1.3296, -4.3497,  ..., -2.7766,  0.2231,  0.2902]],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(m.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d439a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ygpt-jHXQmlC2-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba49873c506e466cc561eca330dfb76e71640cacdd0b7f66b9f171aa33e062dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
